{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f22612b4",
      "metadata": {
        "id": "f22612b4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b9d82a3",
      "metadata": {
        "id": "7b9d82a3"
      },
      "outputs": [],
      "source": [
        "from gpt_model import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbeba028",
      "metadata": {
        "id": "dbeba028",
        "outputId": "7519e145-7e28-4646-aafc-407053cf9f35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 2.9.1\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\"torch\"]\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "964b3f1c",
      "metadata": {
        "id": "964b3f1c"
      },
      "outputs": [],
      "source": [
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"drop_rate\": 0.0,       # Dropout rate\n",
        "    \"qkv_bias\": True        # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "\n",
        "# CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b9c44a9",
      "metadata": {
        "id": "4b9c44a9"
      },
      "outputs": [],
      "source": [
        "# file_name = \"gpt2-small-124M.pth\"\n",
        "file_name = \"gpt2-medium-355M.pth\"\n",
        "# file_name = \"gpt2-large-774M.pth\"\n",
        "# file_name = \"gpt2-xl-1558M.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2285960f",
      "metadata": {
        "id": "2285960f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
        "\n",
        "if not os.path.exists(file_name):\n",
        "    response = requests.get(url, timeout=60)\n",
        "    response.raise_for_status()\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded to {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ffb031",
      "metadata": {
        "id": "f9ffb031"
      },
      "outputs": [],
      "source": [
        "from llms_from_scratch.ch04 import GPTModel\n",
        "# For llms_from_scratch installation instructions, see:\n",
        "# https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "model.load_state_dict(torch.load(file_name, weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a83c5e",
      "metadata": {
        "id": "96a83c5e",
        "outputId": "56927cda-a4c8-4894-b14d-7c1928dde544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you forward, but you must be careful. You must not let your guard down. You must not let your guard down.\"\n",
            "\n",
            "\"I will not\n"
          ]
        }
      ],
      "source": [
        "from llms_from_scratch.ch05 import generate, text_to_token_ids, token_ids_to_text\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
        "    max_new_tokens=30,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    top_k=1,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b952fab9",
      "metadata": {
        "id": "b952fab9",
        "outputId": "4cfc318a-3121-4cb0-ddc6-61da188ed4b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of entries: 1100\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import requests\n",
        "\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "    if not os.path.exists(file_path):\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        text_data = response.text\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "596596b2",
      "metadata": {
        "id": "596596b2",
        "outputId": "d5767546-50b3-444a-e578-4c67748045c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example entry:\n",
            " {'instruction': 'Edit the sentence below to remove any redundant words and make it more concise.', 'input': 'I actually think that', 'output': 'I think.'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Example entry:\\n\", data[173])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a1cfdba",
      "metadata": {
        "id": "5a1cfdba"
      },
      "outputs": [],
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c572c282",
      "metadata": {
        "id": "c572c282",
        "outputId": "f444af79-bb4d-49af-cc62-f4715a630481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify the correct spelling of the following word.\n",
            "\n",
            "### Input:\n",
            "Ocassion\n",
            "\n",
            "### Response:\n",
            "The correct spelling is 'Occasion.'\n"
          ]
        }
      ],
      "source": [
        "model_input = format_input(data[50])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c87a1026",
      "metadata": {
        "id": "c87a1026",
        "outputId": "db3e4aba-5d42-4605-eec2-a7eee9d2323c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n"
          ]
        }
      ],
      "source": [
        "train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]\n",
        "\n",
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15195d77",
      "metadata": {
        "id": "15195d77"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text)\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "608a3c51",
      "metadata": {
        "id": "608a3c51",
        "outputId": "ff692360-b0e7-4ef0-a0d0-05f76f97f690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[50256]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f20c2e6",
      "metadata": {
        "id": "0f20c2e6"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # New: Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c637d340",
      "metadata": {
        "id": "c637d340"
      },
      "outputs": [],
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b40817",
      "metadata": {
        "id": "b6b40817",
        "outputId": "686d62ab-2386-48ff-bc4c-657aef6a9ea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]])\n"
          ]
        }
      ],
      "source": [
        "inputs, targets = custom_collate_fn(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41251478",
      "metadata": {
        "id": "41251478",
        "outputId": "071d7664-4953-4aa9-9661-fb594d2f9db9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.1269)\n"
          ]
        }
      ],
      "source": [
        "logits_1 = torch.tensor(\n",
        "    [[-1.0, 1.0],  # 1st training example\n",
        "     [-0.5, 1.5]]  # 2nd training example\n",
        ")\n",
        "targets_1 = torch.tensor([0, 1])\n",
        "\n",
        "\n",
        "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
        "print(loss_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e63ad1c1",
      "metadata": {
        "id": "e63ad1c1",
        "outputId": "97e04888-f8bf-4858-af4d-5b4f8bac95fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.7936)\n"
          ]
        }
      ],
      "source": [
        "logits_2 = torch.tensor(\n",
        "    [[-1.0, 1.0],\n",
        "     [-0.5, 1.5],\n",
        "     [-0.5, 1.5]]  # New 3rd training example\n",
        ")\n",
        "targets_2 = torch.tensor([0, 1, 1])\n",
        "\n",
        "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
        "print(loss_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf8a5d1",
      "metadata": {
        "id": "3cf8a5d1",
        "outputId": "e94834d0-a413-4d3e-95b0-5d8e9fecb02b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.1269)\n",
            "loss_1 == loss_3: tensor(True)\n"
          ]
        }
      ],
      "source": [
        "targets_3 = torch.tensor([0, 1, -100])\n",
        "\n",
        "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
        "print(loss_3)\n",
        "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ead49c84",
      "metadata": {
        "id": "ead49c84",
        "outputId": "7353a1a9-c338-4104-c2bf-b5c33a078d08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    # Use PyTorch 2.9 or newer for stable mps results\n",
        "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
        "    if (major, minor) >= (2, 9):\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5044ae05",
      "metadata": {
        "id": "5044ae05"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "customized_collate_fn = partial(\n",
        "    custom_collate_fn,\n",
        "    device=device,\n",
        "    allowed_max_length=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5571f85c",
      "metadata": {
        "id": "5571f85c"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97267adc",
      "metadata": {
        "id": "97267adc"
      },
      "outputs": [],
      "source": [
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e41497",
      "metadata": {
        "id": "35e41497",
        "outputId": "6af17c97-256f-4a3f-d2fa-b6657e95571b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loader:\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape, targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "114b5f5b",
      "metadata": {
        "id": "114b5f5b",
        "outputId": "1325f195-53ae-4e3d-c078-85f3026c0f6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
            "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
            "        21017, 46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,\n",
            "          985,   576,    13,   198,   198, 21017, 23412,    25,   198,   464,\n",
            "         5156,   318,   845, 13779,    13,   198,   198, 21017, 18261,    25,\n",
            "          198,   464,  5156,   318,   355, 13779,   355,   257,  4936,    13,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])\n"
          ]
        }
      ],
      "source": [
        "print(inputs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf851b07",
      "metadata": {
        "id": "bf851b07",
        "outputId": "8ff11881-b0d7-4083-dd74-00b3e1a45f99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
            "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
            "        46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,   985,\n",
            "          576,    13,   198,   198, 21017, 23412,    25,   198,   464,  5156,\n",
            "          318,   845, 13779,    13,   198,   198, 21017, 18261,    25,   198,\n",
            "          464,  5156,   318,   355, 13779,   355,   257,  4936,    13, 50256,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])\n"
          ]
        }
      ],
      "source": [
        "print(targets[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aec95f33",
      "metadata": {
        "id": "aec95f33",
        "outputId": "866f99e0-0d6d-45b1-e360-725e5eacdb89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "input_text = format_input(val_data[0])\n",
        "print(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba08c432",
      "metadata": {
        "id": "ba08c432"
      },
      "outputs": [],
      "source": [
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(input_text, tokenizer),\n",
        "    max_new_tokens=35,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    eos_id=50256,\n",
        ")\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee78359",
      "metadata": {
        "id": "4ee78359",
        "outputId": "25a69046-608f-4da5-e4e0-7f99527c2f12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The chef cooks the meal every day.\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Convert the active sentence to passive: 'The chef cooks the\n"
          ]
        }
      ],
      "source": [
        "response_text = (\n",
        "    generated_text[len(input_text):]\n",
        "    .replace(\"### Response:\", \"\")\n",
        "    .strip()\n",
        ")\n",
        "print(response_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c40224",
      "metadata": {
        "id": "40c40224"
      },
      "outputs": [],
      "source": [
        "from llms_from_scratch.ch05 import (\n",
        "    calc_loss_loader,\n",
        "    train_model_simple,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1c4ada2",
      "metadata": {
        "id": "d1c4ada2",
        "outputId": "d552567f-bc7e-48c5-ee0b-1cd2c1771deb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 3.8258957386016847\n",
            "Validation loss: 3.7619208812713625\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "417bb2bd",
      "metadata": {
        "id": "417bb2bd",
        "outputId": "ed91ba04-b42f-4a9a-e504-a075fb6f787d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
            "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.102\n",
            "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.945\n",
            "Ep 1 (Step 000015): Train loss 0.856, Val loss 0.906\n",
            "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
            "Ep 1 (Step 000025): Train loss 0.753, Val loss 0.859\n",
            "Ep 1 (Step 000030): Train loss 0.798, Val loss 0.836\n",
            "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808\n",
            "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
            "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.790\n",
            "Ep 1 (Step 000050): Train loss 0.662, Val loss 0.783\n",
            "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.764\n",
            "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
            "Ep 1 (Step 000065): Train loss 0.652, Val loss 0.735\n",
            "Ep 1 (Step 000070): Train loss 0.532, Val loss 0.729\n",
            "Ep 1 (Step 000075): Train loss 0.569, Val loss 0.729\n",
            "Ep 1 (Step 000080): Train loss 0.605, Val loss 0.725\n",
            "Ep 1 (Step 000085): Train loss 0.509, Val loss 0.709\n",
            "Ep 1 (Step 000090): Train loss 0.562, Val loss 0.691\n",
            "Ep 1 (Step 000095): Train loss 0.500, Val loss 0.681\n",
            "Ep 1 (Step 000100): Train loss 0.502, Val loss 0.677\n",
            "Ep 1 (Step 000105): Train loss 0.564, Val loss 0.670\n",
            "Ep 1 (Step 000110): Train loss 0.555, Val loss 0.667\n",
            "Ep 1 (Step 000115): Train loss 0.508, Val loss 0.664\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
            "Ep 2 (Step 000120): Train loss 0.435, Val loss 0.672\n",
            "Ep 2 (Step 000125): Train loss 0.450, Val loss 0.687\n",
            "Ep 2 (Step 000130): Train loss 0.447, Val loss 0.682\n",
            "Ep 2 (Step 000135): Train loss 0.405, Val loss 0.681\n",
            "Ep 2 (Step 000140): Train loss 0.409, Val loss 0.681\n",
            "Ep 2 (Step 000145): Train loss 0.368, Val loss 0.681\n",
            "Ep 2 (Step 000150): Train loss 0.381, Val loss 0.676\n",
            "Ep 2 (Step 000155): Train loss 0.412, Val loss 0.676\n",
            "Ep 2 (Step 000160): Train loss 0.415, Val loss 0.684\n",
            "Ep 2 (Step 000165): Train loss 0.379, Val loss 0.686\n",
            "Ep 2 (Step 000170): Train loss 0.323, Val loss 0.683\n",
            "Ep 2 (Step 000175): Train loss 0.337, Val loss 0.671\n",
            "Ep 2 (Step 000180): Train loss 0.392, Val loss 0.658\n",
            "Ep 2 (Step 000185): Train loss 0.415, Val loss 0.660\n",
            "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.651\n",
            "Ep 2 (Step 000195): Train loss 0.330, Val loss 0.638\n",
            "Ep 2 (Step 000200): Train loss 0.310, Val loss 0.638\n",
            "Ep 2 (Step 000205): Train loss 0.351, Val loss 0.634\n",
            "Ep 2 (Step 000210): Train loss 0.366, Val loss 0.632\n",
            "Ep 2 (Step 000215): Train loss 0.396, Val loss 0.636\n",
            "Ep 2 (Step 000220): Train loss 0.300, Val loss 0.648\n",
            "Ep 2 (Step 000225): Train loss 0.348, Val loss 0.661\n",
            "Ep 2 (Step 000230): Train loss 0.294, Val loss 0.657\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
            "Training completed in 52.60 minutes.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "940a6a27",
      "metadata": {
        "id": "940a6a27",
        "outputId": "04823125-c17d-4641-b90f-abccf14853cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence using a simile.\n",
            "\n",
            "### Input:\n",
            "The car is very fast.\n",
            "\n",
            "Correct response:\n",
            ">> The car is as fast as lightning.\n",
            "\n",
            "Model response:\n",
            ">> The car is as fast as a bullet.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What type of cloud is typically associated with thunderstorms?\n",
            "\n",
            "Correct response:\n",
            ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
            "\n",
            "Model response:\n",
            ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name the author of 'Pride and Prejudice'.\n",
            "\n",
            "Correct response:\n",
            ">> Jane Austen.\n",
            "\n",
            "Model response:\n",
            ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
            "-------------------------------------\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "for entry in test_data[:3]:\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        ")\n",
        "\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d57ae490",
      "metadata": {
        "id": "d57ae490",
        "outputId": "1636342c-a1ae-45fe-b137-12c7e2b4977b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [20:17<00:00, 11.07s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efcdc3c9",
      "metadata": {
        "id": "efcdc3c9",
        "outputId": "f1c0053e-ad3c-437e-d594-c9075abdeb42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a bullet.'}\n"
          ]
        }
      ],
      "source": [
        "print(test_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab342ab4",
      "metadata": {
        "id": "ab342ab4",
        "outputId": "e7c1d86c-b6c2-4aea-c0a6-522b9fd35851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved as gpt2-medium355M-sft.pth\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
        "torch.save(model.state_dict(), file_name)\n",
        "print(f\"Model saved as {file_name}\")\n",
        "\n",
        "# Load model via\n",
        "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a093a2fc",
      "metadata": {
        "id": "a093a2fc",
        "outputId": "0a8579e6-6672-40e7-f818-2faf3a956280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ollama running: True\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "\n",
        "def check_if_running(process_name):\n",
        "    running = False\n",
        "    for proc in psutil.process_iter([\"name\"]):\n",
        "        if process_name in proc.info[\"name\"]:\n",
        "            running = True\n",
        "            break\n",
        "    return running\n",
        "\n",
        "ollama_running = check_if_running(\"ollama\")\n",
        "\n",
        "if not ollama_running:\n",
        "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
        "print(\"Ollama running:\", check_if_running(\"ollama\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82600af3",
      "metadata": {
        "id": "82600af3",
        "outputId": "e2981cfc-2228-4335-9bc3-3d855a791aad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
            "\n",
            "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
            "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in many llama diets.\n",
            "3. Grains: Oats, barley, and corn are common grains that llamas enjoy.\n",
            "4. Fruits and vegetables: Llamas may also eat fruits like apples, carrots, and sweet potatoes, as well as leafy greens like kale and spinach.\n",
            "5. Minerals: Llamas need access to minerals like calcium, phosphorus, and salt to stay healthy.\n",
            "\n",
            "In the wild, llamas might also eat:\n",
            "\n",
            "1. Leaves: They'll munch on leaves from trees and shrubs, like willow or cedar.\n",
            "2. Bark: In some cases, they may eat the bark of certain trees, like aspen or birch.\n",
            "3. Mosses: Llamas have been known to graze on mosses that grow in their natural habitats.\n",
            "\n",
            "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and supplements specifically formulated for llamas. It's essential to ensure that your llama is receiving the nutrients it needs to stay healthy and thrive.\n"
          ]
        }
      ],
      "source": [
        "import requests  # noqa: F811\n",
        "# import urllib.request\n",
        "\n",
        "def query_model(\n",
        "    prompt,\n",
        "    model=\"llama3\",\n",
        "    # If you used OLLAMA_HOST=127.0.0.1:11435 ollama serve\n",
        "    # update the address from 11434 to 11435\n",
        "    url=\"http://localhost:11434/api/chat\"\n",
        "):\n",
        "    # Create the data payload as a dictionary\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"options\": {     # Settings below are required for deterministic responses\n",
        "            \"seed\": 123,\n",
        "            \"temperature\": 0,\n",
        "            \"num_ctx\": 2048\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
        "    payload = json.dumps(data).encode(\"utf-8\")\n",
        "\n",
        "    # Create a request object, setting the method to POST and adding necessary headers\n",
        "    request = urllib.request.Request(\n",
        "        url,\n",
        "        data=payload,\n",
        "        method=\"POST\"\n",
        "    )\n",
        "    request.add_header(\"Content-Type\", \"application/json\")\n",
        "\n",
        "    # Send the request and capture the response\n",
        "    response_data = \"\"\n",
        "    with urllib.request.urlopen(request) as response:\n",
        "        # Read and decode the response\n",
        "        while True:\n",
        "            line = response.readline().decode(\"utf-8\")\n",
        "            if not line:\n",
        "                break\n",
        "            response_json = json.loads(line)\n",
        "            response_data += response_json[\"message\"][\"content\"]\n",
        "\n",
        "    return response_data\n",
        "    \"\"\"\n",
        "\n",
        "    # The book originally used the commented-out above, which is based\n",
        "    # on urllib. It works generally fine, but some readers reported\n",
        "    # issues with using urlib when using a (company) VPN.\n",
        "    # The code below uses the requests library, which doesn't seem\n",
        "    # to have these issues.\n",
        "\n",
        "    # Send the POST request\n",
        "    with requests.post(url, json=data, stream=True, timeout=30) as r:\n",
        "        r.raise_for_status()\n",
        "        response_data = \"\"\n",
        "        for line in r.iter_lines(decode_unicode=True):\n",
        "            if not line:\n",
        "                continue\n",
        "            response_json = json.loads(line)\n",
        "            if \"message\" in response_json:\n",
        "                response_data += response_json[\"message\"][\"content\"]\n",
        "\n",
        "    return response_data\n",
        "\n",
        "\n",
        "model = \"llama3\"\n",
        "result = query_model(\"What do Llamas eat?\", model)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fb08076",
      "metadata": {
        "id": "9fb08076"
      },
      "outputs": [],
      "source": [
        "for entry in test_data[:3]:\n",
        "    prompt = (\n",
        "        f\"Given the input `{format_input(entry)}` \"\n",
        "        f\"and correct output `{entry['output']}`, \"\n",
        "        f\"score the model response `{entry['model_response']}`\"\n",
        "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "    )\n",
        "    print(\"\\nDataset response:\")\n",
        "    print(\">>\", entry['output'])\n",
        "    print(\"\\nModel response:\")\n",
        "    print(\">>\", entry[\"model_response\"])\n",
        "    print(\"\\nScore:\")\n",
        "    print(\">>\", query_model(prompt))\n",
        "    print(\"\\n-------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da4d0ff2",
      "metadata": {
        "id": "da4d0ff2"
      },
      "outputs": [],
      "source": [
        "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
        "    scores = []\n",
        "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
        "        prompt = (\n",
        "            f\"Given the input `{format_input(entry)}` \"\n",
        "            f\"and correct output `{entry['output']}`, \"\n",
        "            f\"score the model response `{entry[json_key]}`\"\n",
        "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "            f\"Respond with the integer number only.\"\n",
        "        )\n",
        "        score = query_model(prompt, model)\n",
        "        try:\n",
        "            scores.append(int(score))\n",
        "        except ValueError:\n",
        "            print(f\"Could not convert score: {score}\")\n",
        "            continue\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "scores = generate_model_scores(test_data, \"model_response\")\n",
        "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
        "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a645232e",
      "metadata": {
        "id": "a645232e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d28463",
      "metadata": {
        "id": "01d28463"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TensorFlow (tfenv)",
      "language": "python",
      "name": "tfenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}